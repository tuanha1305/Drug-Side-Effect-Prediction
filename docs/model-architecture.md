Model nÃ y lÃ  má»™t **Transformer-based hybrid architecture** Ä‘Æ°á»£c thiáº¿t káº¿ chuyÃªn biá»‡t cho bÃ i toÃ¡n **drugâ€“side effect prediction** (dá»± Ä‘oÃ¡n tÃ¡c dá»¥ng phá»¥ cá»§a thuá»‘c).
NÃ³ káº¿t há»£p giá»¯a **representation learning (Transformer)** vÃ  **interaction modeling (Outer Product + CNN)** Ä‘á»ƒ náº¯m báº¯t má»‘i quan há»‡ phá»©c táº¡p giá»¯a **thuá»‘c** vÃ  **tÃ¡c dá»¥ng phá»¥**.

---

## ğŸ§© 1. **Overall Pipeline**

```
Drug (SMILES/token seq) â”€â”
                         â”œâ”€> Drug Encoder â”€â”
                         â”‚                 â”‚
                         â”‚                 â”‚
Side Effect (term seq) â”€â”€â”˜                 â”œâ”€> Interaction (outer product + CNN) â†’ MLP â†’ Score
                                           â”‚
                                           â””â”€> (Optional) Cross Attention
```

---

## ğŸ”¹ 2. **Embedding Layers**

### `self.emb_drug` & `self.emb_side`

Hai lá»›p embedding nÃ y chuyá»ƒn Ä‘á»•i chá»‰ sá»‘ token (thuá»‘c hoáº·c tÃ¡c dá»¥ng phá»¥) thÃ nh vector liÃªn tá»¥c `hidden_size` chiá»u, Ä‘á»“ng thá»i thÃªm positional embedding vÃ  dropout.

* Input: `[batch, seq_len]`
* Output: `[batch, seq_len, hidden_size]`

> Má»—i â€œthuá»‘câ€ vÃ  â€œtÃ¡c dá»¥ng phá»¥â€ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° chuá»—i token (vd: substructures hoáº·c words), sau Ä‘Ã³ Ä‘Æ°á»£c Ã¡nh xáº¡ vÃ o khÃ´ng gian vector.

---

## ğŸ”¹ 3. **Transformer Encoders**

### `self.encoder_drug` vÃ  `self.encoder_side`

Hai khá»‘i encoder Ä‘á»™c láº­p â€” má»—i bÃªn má»™t Transformer nhiá»u táº§ng (`Encoder_MultipleLayers`).

ChÃºng há»c **biá»ƒu diá»…n ngá»¯ cáº£nh ná»™i táº¡i**:

* Trong chuá»—i thuá»‘c: mÃ´ hÃ¬nh há»c má»‘i quan há»‡ giá»¯a cÃ¡c substructures.
* Trong chuá»—i side effect: mÃ´ hÃ¬nh há»c ngá»¯ nghÄ©a giá»¯a cÃ¡c thÃ nh pháº§n cá»§a mÃ´ táº£ tÃ¡c dá»¥ng phá»¥.

Cáº¥u trÃºc má»—i encoder layer gá»“m:

* Multi-head self-attention
* Feed-forward network
* Residual + LayerNorm
* (TÃ¹y chá»n: FlashAttention / SDPA / gradient checkpointing)

> Má»¥c tiÃªu: há»c Ä‘Æ°á»£c embedding â€œcáº¥p cÃ¢uâ€ (contextualized embeddings).

---

## ğŸ”¹ 4. **Optional Cross-Attention Encoder**

Náº¿u `config.use_cross_attention=True`, mÃ´ hÃ¬nh thÃªm 1 táº§ng Transformer Ä‘á»ƒ **cho phÃ©p hai chuá»—i tÆ°Æ¡ng tÃ¡c trá»±c tiáº¿p á»Ÿ má»©c attention**.

Cá»¥ thá»ƒ:

```python
combined = torch.cat([drug_encoded, se_encoded], dim=1)
combined = cross_attention_encoder(combined)
```

â†’ giÃºp mÃ´ hÃ¬nh hiá»ƒu Ä‘Æ°á»£c **má»‘i quan há»‡ giá»¯a tá»«ng token cá»§a thuá»‘c vÃ  tá»«ng token cá»§a side effect**, thay vÃ¬ xá»­ lÃ½ tÃ¡ch biá»‡t.

---

## ğŸ”¹ 5. **Interaction Layer (Outer Product + CNN)**

ÄÃ¢y lÃ  pháº§n cá»‘t lÃµi Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a **tÆ°Æ¡ng tÃ¡c hai chiá»u giá»¯a thuá»‘c vÃ  tÃ¡c dá»¥ng phá»¥**.

### âœ³ CÆ¡ cháº¿:

1. Má»Ÿ rá»™ng chiá»u Ä‘á»ƒ broadcast:

   ```python
   drug_aug = [batch, drug_len, 1, hidden]
   se_aug   = [batch, 1, se_len, hidden]
   ```

2. Outer product:

   ```python
   interaction = drug_aug * se_aug
   # => [batch, drug_len, se_len, hidden]
   ```

   â†’ tÆ°Æ¡ng tá»± nhÆ° táº¡o má»™t â€œma tráº­n tÆ°Æ¡ng tÃ¡câ€ giá»¯a má»—i token thuá»‘c vÃ  má»—i token side effect.

3. HoÃ¡n vá»‹ cho CNN:

   ```python
   interaction = interaction.permute(0, 3, 1, 2)
   # => [batch, hidden, drug_len, se_len]
   ```

4. Giáº£m chiá»u hidden báº±ng tá»•ng:

   ```python
   interaction = torch.sum(interaction, dim=1, keepdim=True)
   # => [batch, 1, drug_len, se_len]
   ```

5. Ãp dá»¥ng CNN Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng cá»¥c bá»™:

   ```python
   interaction_features = self.interaction_cnn(interaction)
   ```

   â†’ CNN giÃºp há»c cÃ¡c **pattern cá»¥c bá»™** giá»¯a subsequences (vd: motif liÃªn quan Ä‘áº¿n tÃ¡c dá»¥ng phá»¥ cá»¥ thá»ƒ).

6. Flatten Ä‘á»ƒ Ä‘Æ°a vÃ o decoder.

---

## ğŸ”¹ 6. **Decoder (MLP)**

Pháº§n cuá»‘i lÃ  má»™t máº¡ng **Multi-Layer Perceptron** gá»“m nhiá»u táº§ng linear + ReLU (+ BatchNorm náº¿u báº­t).

Nhiá»‡m vá»¥:

* Nháº­n Ä‘áº§u vÃ o lÃ  vector flattened tá»« táº§ng CNN.
* Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t (hoáº·c score liÃªn tá»¥c) cá»§a â€œthuá»‘c cÃ³ gÃ¢y tÃ¡c dá»¥ng phá»¥ nÃ y hay khÃ´ngâ€.

Output:

```python
score = self.decoder(interaction_flat)
# [batch, 1]
```

---

## ğŸ”¹ 7. **Training Objective**

TÃ¹y dataset, loss thÆ°á»ng lÃ :

* **Binary cross-entropy (BCE)**: náº¿u bÃ i toÃ¡n nhá»‹ phÃ¢n (cÃ³/khÃ´ng tÃ¡c dá»¥ng phá»¥)
* **Regression loss (MSE)**: náº¿u label lÃ  má»©c Ä‘á»™ nghiÃªm trá»ng

---

## ğŸ”¹ 8. **Attention Mask**

HÃ m `_create_attention_mask` biáº¿n mask `[batch, seq_len]` thÃ nh `[batch, 1, 1, seq_len]` vá»›i:

* 1 â†’ token há»£p lá»‡
* 0 â†’ padding (bá»‹ pháº¡t `-10000` trong attention logits)

---

## ğŸ”¹ 9. **Inference & Feature Extraction**

`get_embeddings()` cho phÃ©p trÃ­ch xuáº¥t embedding Ä‘Ã£ encode cá»§a thuá»‘c & side effect mÃ  khÃ´ng cáº§n tÃ­nh dá»± Ä‘oÃ¡n.
â†’ há»¯u Ã­ch khi dÃ¹ng lÃ m feature cho downstream task (vd clustering, visualization).

---

## ğŸ”¹ 10. **Parameter Breakdown**

`count_parameters()` thá»‘ng kÃª:

* Tá»•ng sá»‘ tham sá»‘
* Tham sá»‘ trainable
* Sá»‘ tham sá»‘ riÃªng cá»§a tá»«ng module (encoder, decoder)

---

## âš™ï¸ TÃ³m táº¯t dÃ²ng cháº£y dá»¯ liá»‡u

```
drug_seq --> Embedding --> Transformer --> drug_encoded â”
                                                         â”œâ”€> Outer Product -> CNN -> Flatten -> MLP -> Score
side_seq --> Embedding --> Transformer --> se_encoded   â”˜
```

TÃ¹y `config`, cÃ³ thá»ƒ cÃ³ thÃªm cross-attention giá»¯a `drug_encoded` vÃ  `se_encoded`.

---

## ğŸ§  Ã nghÄ©a mÃ´ hÃ¬nh

* **Drug encoder** há»c representation tá»« SMILES hoáº·c substructure cá»§a thuá»‘c.
* **Side effect encoder** há»c semantic embedding cá»§a tÃ¡c dá»¥ng phá»¥.
* **Outer product + CNN** há»c cÃ¡ch mÃ  cÃ¡c Ä‘áº·c trÆ°ng cá»§a thuá»‘c â€œkáº¿t há»£pâ€ vá»›i nhau Ä‘á»ƒ gÃ¢y ra má»™t tÃ¡c dá»¥ng phá»¥ cá»¥ thá»ƒ.
* **MLP decoder** tá»•ng há»£p thÃ´ng tin thÃ nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng.
